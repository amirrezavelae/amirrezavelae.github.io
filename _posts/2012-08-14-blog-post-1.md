---
title: "Shāh māt"
date: 2025-09-08
permalink: /posts/shah-mat/
description: "Exploring the intersection of chess, AI, and graphical models."
tags:
  - cool posts
---

# Chess and AI

## "Shāh māt" means "the king is dead" in Persian, and it’s the term used in chess when a player checkmates their opponent.

<a href="https://taketaketake.com/" target="_blank"> Take Take Take</a> is an innovative platform dedicated to tracking chess tournaments and providing live updates on ongoing games. Recently, the site hosted an exciting contest to identify the best AI chess player, with Kaggle facilitating the competition. The challenge? To see which large language model (LLM) could outperform other LLMs in chess.

As both an AI enthusiast and a chess fan, I’ve long been curious about why large language models (LLMs) struggle with chess, especially when compared to other specialized AIs. (For a deeper dive into this, check out this fascinating video from <a href="https://www.youtube.com/watch?v=iWhlrkfJrCQ&ab_channel=GothamChess" target="_blank">GothamChess</a>.) So this contest was a perfect opportunity to explore this question further.

<figure style="display: flex; flex-direction: column; align-items: center; margin: auto;">
  <img src="/images/gambit.webp" 
       alt="Queen's Gambit" 
       style="width: 600px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
  <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
    Queen's Gambit, a popular Netflix series that sparked a chess renaissance.
  </figcaption>
</figure>


## Chess: A Game of Strategy and Phases

In chess, the game is divided into three main phases: **opening**, **middlegame**, and **endgame**.

### **Opening**

The **opening** refers to the first few moves where players focus on developing pieces, controlling the center, and ensuring the king's safety. These strategies are so played that they can be memorized, and many players have their favorite openings. The goal is to set up a strong position for the middlegame. There several well-known openings, such as the King's Indian Defense, Sicilian Defense, and Queen's Gambit, each with its own strategies and tactics.

<p align="center">
  <img src="/images/meme.jpeg" alt="Chess meme" style="width: 600px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
</p>

### **Middlegame**

The **middlegame** begins once the pieces are developed. This phase is all about tactical battles, with players trying to create weaknesses, attack the opponent's king, or gain material advantage. It’s where most of the action happens. In other words, the middlegame is where players try to outmaneuver each other, using tactics like forks, pins, and skewers to gain an advantage. There is no straightforward way to play the middlegame, as it depends on the position of the pieces and the players' strategies. Players often rely on **their intuition** and **experience** to navigate this complex phase.

### **Endgame**

The **endgame** comes when there are fewer pieces left, and players focus on promoting pawns and checkmating the opponent. Precision is key, and mastering endgame techniques can turn a slight advantage into victory. Actually, in high-level chess, the endgame is often where games has already been decided, as players with a slight advantage can convert it into a win. So basicly, it's almost rare to see a game go to checkmate in the endgame phase, as players often resign when they realize they cannot win.

<figure style="display: flex; flex-direction: column; align-items: center; margin: auto;">
  <img src="/images/magnuskasparov.jpg" 
       alt="Magnus Carlsen vs Garry Kasparov" 
       style="width: 600px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
  <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
    Magnus Carlsen facing Garry Kasparov in the Reykjavik Rapid tournament. At 13, Carlsen drew the game against the legendary Kasparov.
  </figcaption>
</figure>


## Why do I care about the chess phases?

Sergey Levine is a professor at UC Berkeley and a leading figure in the field of AI. He has a tutorial on graphical models and how one can use it reinforcement learning and what are some intuitions behind it. (you can see the slides I designed to summarize the tutorial for my Graphical Models course <a href="https://github.com/amirrezavelae/Graphical-Models/blob/main/Presentation/Presentation.pdf" target="_blank">here</a>. The original paper is available <a href="https://arxiv.org/abs/1805.00909" target="_blank">here</a>.) In this tutorial, he discusses how graphical representations can be used to solve reinforcement learning problems, using sum product algorithms and belief propagation.

## Short explaination of graphical models

Graphical models are a way to represent complex relationships between variables using graphs. In reinforcement learning, they can be used to model the interactions between states, actions, and rewards. The key idea is to use **messages** that flow through the graph to compute probabilities and make decisions. Sergey defines some variables in the graphical model as optimality variables, which are the variables that shows a state is optimal or not. How we define these variables is by using exponential functions of the rewards.

### Reinforcement Learning as Inference in Graphical Models

Imagine reinforcement learning as an inference problem in a “decision forest,” where at each time step \(t\) a tiny lantern—our optimality variable \(O_t\)—glows brighter for actions that promise higher reward. Formally, we introduce a binary variable \(O_t\) at each step and define

\[
p(O_t = 1 \mid s_t, a_t) \propto \exp\bigl(r(s_t, a_t)\bigr).
\]

Conditioning on \(O\_{1:T}=1\) forces the model to follow only the most promising trajectories. The resulting chain‐structured graphical model links
\[
s_1 \to a_1 \to O_1 \to s_2 \to a_2 \to O_2 \to \dots \to s_T \to a_T \to O_T,
\]
weaving states, actions, and optimality flags into a single probabilistic tapestry. Inference in this model—asking “Which path shines brightest?”—directly yields the soft‐optimal policy by preferring high‐reward branches, thus transforming the control problem into a journey of inference through the decision forest.

### Inference → Control

To derive the optimal policy, we treat decision making as an inference problem. In this view, we want the action distribution that maximizes the probability of selecting only “optimal” moves over an entire trajectory, i.e.

$$
\pi^*(a_t \mid s_t) = p(a_t \mid s_t, O_{t:T}=1),
$$

where \(O\_{t:T}=1\) indicates that every step from \(t\) to \(T\) is optimal. By running the sum-product algorithm on our augmented graphical model, we compute backward messages \(\beta_t(s_t)\) that play the same role as value functions, accumulating future rewards for each state. These messages combine with immediate rewards to form the state–action values \(Q(s_t, a_t)\). The resulting policy emerges naturally as a softmax over those values:

$$
\pi^*(a_t \mid s_t) = \frac{\exp\bigl(Q(s_t, a_t)\bigr)}{\exp\bigl(V(s_t)\bigr)},
$$

where \(V(s_t)\) is the log-normalizer ensuring the policy sums to one.

## Why It Matters

Framing control as inference unifies planning, decision making, and probabilistic reasoning into a single coherent framework. Stochastic policies arise organically through the softmax, capturing uncertainty and encouraging exploration without ad-hoc tricks. This viewpoint also yields maximum-entropy reinforcement learning objectives, which balance reward maximization with thorough exploration. In practice, inference-driven control often outperforms simple heuristics like ε-greedy by systematically leveraging model structure and probability flow to guide more efficient exploration.

### State Marginals and Their Importance

In graphical models, **state marginals** represent the probability distribution of the state at each time step, independent of the actions or future states. They are obtained by considering the intersection of the forward and backward messages:

1. **Forward messages (αₜ)**: Capture the probability of reaching a state from the initial state, factoring in the rewards. These represent the states with a high probability of being reached.

2. **Backward messages (βₜ)**: Capture the probability of reaching the goal from the current state. These represent the states that are likely to reach the goal.

When these two messages intersect, the **state marginals** describe the states that are most probable at each time step, given both the past (via backward messages) and the future (via forward messages). The intersection of these cones, as seen in the figure, identifies the most optimal trajectory that maximizes both reaching the goal and achieving high rewards.

There is an amazing picture in the slides of CS 285 Berkeley course that shows how forward and backward massage passing can mimic our thinking process.

<figure style="display: flex; flex-direction: column; align-items: center;">
  <img src="/images/statemarginals.png" 
       alt="State Marginals" 
       style="width: 600px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
  <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
    State Marginals in Graphical Models
  </figcaption>
</figure>


Look closer—doesn't it resemble the phases of chess? Isn't this wonderful? The forward messages represent how we think about the opening phase, where we consider future moves and try to set up a strong position. The backward messages represent how we think about the endgame phase, where we focus on the final moves and checkmate the opponent. The intersection of these two messages represents the middlegame phase, where we try to outmaneuver our opponent and gain an advantage.

I love this representation of chess phases as a graphical model, as it shows how our thinking process can be modeled as a probabilistic inference problem; and it is yet another reason why you should love reinforcement learning and graphical models. I think this is a beautiful example of why language models may not be the best fit for human reasoning in complex tasks, as they often lack structured reasoning. Instead, using reinforcement learning and other techniques that leverage graphical models can provide a more accurate representation of how we think and make decisions in complex scenarios like chess.

### Stockfish, why RL is good I guess

Stockfish, one of the most formidable open-source chess engines, still relies on a classic alpha–beta search at its core. Its true breakthrough came with the adoption of Neural Network Unified Evaluation (NNUE), a compact neural network first developed for Shogi and later integrated into Stockfish 12. The engine generates millions of positions by playing against itself and uses those self-play games to train the NNUE weights via Temporal-Difference Learning, so that the network’s evaluations more closely match the outcomes of deep search.

What sets NNUE apart from larger convolutional models is its reliance on sparse piece-square features fed into a single hidden layer—this design makes it fast enough to be queried at every leaf node without slowing down the alpha–beta framework. During search, Stockfish merges these learned evaluations with its long-standing heuristics to guide move ordering and pruning. In combining exhaustive tree search with a lightweight, learned evaluation, Stockfish demonstrates how reinforcement learning can dramatically boost engine strength while preserving the proven search techniques that have powered chess engines for decades.

<p align="center">
  <img src="/images/meme2.webp" alt="Stockfish" style="width: 400px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
</p>


Hit me up if you want to discuss this topic further or have any suggestions for improvements. I would love to hear your thoughts and ideas.
