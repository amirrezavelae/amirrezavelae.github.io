---
title: "Shāh māt"
date: 2025-09-08
permalink: /posts/2012/08/blog-post-1/
tags:
  - cool posts
---

# Chess and AI

## "Shāh māt" means "the king is dead" in Persian, and it’s the term used in chess when a player checkmates their opponent.

<a href="https://taketaketake.com/" target="_blank"> Take Take Take</a> is an innovative platform dedicated to tracking chess tournaments and providing live updates on ongoing games. Recently, the site hosted an exciting contest to identify the best AI chess player, with Kaggle facilitating the competition. The challenge? To build an AI capable of playing chess at a grandmaster level.

As both an AI enthusiast and a chess fan, I’ve long been curious about why large language models (LLMs) struggle with chess, especially when compared to other specialized AIs. (For a deeper dive into this, check out this fascinating video from <a href="https://www.youtube.com/watch?v=iWhlrkfJrCQ&ab_channel=GothamChess" target="_blank">GothamChess</a>.) So this contest was a perfect opportunity to explore this question further.

<p align="center">
  <img src="/images/gambit.webp" alt="Queen's Gambit" style="width: 500px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
</p>

## Chess: A Game of Strategy and Phases

In chess, the game is divided into three main phases: **opening**, **middlegame**, and **endgame**.

### **Opening**

The **opening** refers to the first few moves where players focus on developing pieces, controlling the center, and ensuring the king's safety. These strategies are so played that they can be memorized, and many players have their favorite openings. The goal is to set up a strong position for the middlegame. There several well-known openings, such as the King's Indian Defense, Sicilian Defense, and Queen's Gambit, each with its own strategies and tactics.

### **Middlegame**

The **middlegame** begins once the pieces are developed. This phase is all about tactical battles, with players trying to create weaknesses, attack the opponent's king, or gain material advantage. It’s where most of the action happens. In other words, the middlegame is where players try to outmaneuver each other, using tactics like forks, pins, and skewers to gain an advantage. There is no straightforward way to play the middlegame, as it depends on the position of the pieces and the players' strategies. Players often rely on **their intuition** and **experience** to navigate this complex phase.

### **Endgame**

The **endgame** comes when there are fewer pieces left, and players focus on promoting pawns and checkmating the opponent. Precision is key, and mastering endgame techniques can turn a slight advantage into victory. Actually, in high-level chess, the endgame is often where games has already been decided, as players with a slight advantage can convert it into a win. So basicly, it's almost rare to see a game go to checkmate in the endgame phase, as players often resign when they realize they cannot win.

<p align="center">
  <img src="/images/magnuskasparov.jpg" alt="Magnus Carlsen vs Garry Kasparov" style="width: 500px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
</p>

## Why do I care about the chess phases?

Sergey Levine is a professor at UC Berkeley and a leading figure in the field of AI. He has a tutorial on graphical models and how one can use it reinforcement learning and what are some intuitions behind it. (you can see the slides I designed to summarize the tutorial for my Graphical Models course <a href="https://github.com/amirrezavelae/Graphical-Models/blob/main/Presentation/Presentation.pdf" target="_blank">here</a>. The original paper is available <a href="https://arxiv.org/abs/1805.00909" target="_blank">here</a>.) In this tutorial, he discusses how graphical representations can be used to solve reinforcement learning problems, using sum product algorithms and belief propagation.

## Short explaination of graphical models

Graphical models are a way to represent complex relationships between variables using graphs. In reinforcement learning, they can be used to model the interactions between states, actions, and rewards. The key idea is to use **messages** that flow through the graph to compute probabilities and make decisions. Sergey defines some variables in the graphical model as optimality variables, which are the variables that shows a state is optimal or not. How we define these variables is by using exponential functions of the rewards.

### Reinforcement Learning as Inference in Graphical Models

**Core idea:** Treat RL control as inference in a probabilistic graphical model (PGM). We augment the standard MDP with **optimality variables** \(O_t\):

$$
p(O_t = 1 | s_t, a_t) \propto \exp(r(s_t, a_t))
$$

Imagine walking through every decision as if following a glowing trail: the reward function becomes the light that guides us, making actions with higher payoff shine more brightly in the probability landscape. We introduce a binary flag \\(O_t\\) at each time step to indicate whether the chosen move is truly optimal—pushed to one during inference, these flags force the model to follow only the most promising trajectories. Under the hood, this structure resembles a simple chain, where each state s_t gives rise to an action a_t and an adjacent marker \\(O_t\\) that signals optimality. By weaving reward, optimality, and chain-like dependencies into a single graphical tapestry, we transform reinforcement learning into a journey of inference—seeking the highest-reward path through every fork in the road.

### Inference → Control

To derive the optimal policy, we treat decision making as an inference problem. In this view, we want the action distribution that maximizes the probability of selecting only “optimal” moves over an entire trajectory, i.e.

$$
\pi^*(a_t \mid s_t) = p(a_t \mid s_t, O_{t:T}=1),
$$

where \(O\_{t:T}=1\) indicates that every step from \(t\) to \(T\) is optimal. By running the sum-product algorithm on our augmented graphical model, we compute backward messages \(\beta_t(s_t)\) that play the same role as value functions, accumulating future rewards for each state. These messages combine with immediate rewards to form the state–action values \(Q(s_t, a_t)\). The resulting policy emerges naturally as a softmax over those values:

$$
\pi^*(a_t \mid s_t) = \frac{\exp\bigl(Q(s_t, a_t)\bigr)}{\exp\bigl(V(s_t)\bigr)},
$$

where \(V(s_t)\) is the log-normalizer ensuring the policy sums to one.

## Why It Matters

Framing control as inference unifies planning, decision making, and probabilistic reasoning into a single coherent framework. Stochastic policies arise organically through the softmax, capturing uncertainty and encouraging exploration without ad-hoc tricks. This viewpoint also yields maximum-entropy reinforcement learning objectives, which balance reward maximization with thorough exploration. In practice, inference-driven control often outperforms simple heuristics like ε-greedy by systematically leveraging model structure and probability flow to guide more efficient exploration.

### State Marginals and Their Importance

In graphical models, **state marginals** represent the probability distribution of the state at each time step, independent of the actions or future states. They are obtained by considering the intersection of the forward and backward messages:

1. **Forward messages (αₜ)**: Capture the probability of reaching a state from the initial state, factoring in the rewards. These represent the states with a high probability of being reached.

2. **Backward messages (βₜ)**: Capture the probability of reaching the goal from the current state. These represent the states that are likely to reach the goal.

When these two messages intersect, the **state marginals** describe the states that are most probable at each time step, given both the past (via backward messages) and the future (via forward messages). The intersection of these cones, as seen in the figure, identifies the most optimal trajectory that maximizes both reaching the goal and achieving high rewards.

There is an amazing picture in the slides of CS 285 Berkeley course that shows how forward and backward massage passing can mimic our thinking process.

<p align="center">
  <img src="/images/statemarginals.png" alt="State Marginals" style="width: 500px; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
</p>

Look closer, doesn't it look like the phases of chess? Isn't this wonderful? The forward messages represent how we think about the opening phase, where we consider the future moves and try to set up a strong position. The backward messages represent how we think about the endgame phase, where we focus on the final moves and checkmating the opponent. The intersection of these two messages represents the middlegame phase, where we try to outmaneuver our opponent and gain an advantage.

I love this representation of chess phases as a graphical model, as it shows how our thinking process can be modeled as a probabilistic inference problem; ant it is yes another reason why you should love reinforcement learning and graphical models.

HMU if you want to discuss this topic further or have any suggestions for improvements. I would love to hear your thoughts and ideas.
